---
title: "Vorhersage der Stromerzeugung basierend auf Self Learning Activation Functions und Liquid Neuronal Networks"
abstract: "

Bei der Vorhersage von Zeitreihen kann die Anpassung an die aktuelle Situation 
eine große Rolle spielen. So i.d.r. ob z.B. Umwelteinflüsse oder 
Pandemien eine große Veränderung in einer Zeitreihe herbeiführen. 
Der erste Ansatz für eine Vorhersage von Zeitreihen basiert in dieser Arbeit
auf den Liquid Neuronal Networks (LNN). Die LNNs sind dynamisch und passen 
sich an die aktuelle Situation an, lernen also immer weiter.
Als eine Ergänzung zu den LNNs wir die Self Learning Activation Function (SLAF) implementiert.
Diese approximiert die Parametrierung der richtigen Aktivierungsfunktion.
Das Neuronale Netz wird auf den Datensatz der Stromerzeugung in den Jahren 2004-2018 
vom Unternehmen American Electric Power angewandt. 
"
keywords: "SLAF, LNN, Forecasting, Electric Power"

course: Neuronal Network and Deep Learning (Prof. Dr. Thomas Kopinski und Felix Neubürger)
supervisor: Prof. Dr. Thomas Kopinski und Felix Neubürger
city: Meschede

# List of Authors
author:
- familyname: Krilov
  othernames: Vitali
  address: "MatNr: 30329089"
  qualifications: "Data Science (Ms, 4. Semester)"
  email: curie.marie@fh-swf.de
  correspondingauthor: true
- familyname: Stasenko
  othernames: Vladislav
  address: "MatNr: 87654321"
  qualifications: "Data Science (Ms, 4. Semester)"
  email: curie.pierre@fh-swf.de

# Language Options
german: true # German Dummy Text
lang: de-de   # Text Language: en-gb, en-us, de-de

# Indexes
toc: true     # Table of Contents
lot: false    # List of Tables
lof: false    # List of Figures

# Output Options
bibliography: references.bib
biblio-style: authoryear-comp
blind: false
cover: true
checklist: false
output:
  fhswf::seminarpaper:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
knit: fhswf::render_seminarpaper

header-includes:
- \usepackage{hyperref}
- \usepackage{caption}
- \usepackage{csquotes}
- \usepackage{graphicx}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=FALSE, messages=FALSE, warning=FALSE, 
                      attr.source='.numberLines', singlespacing = TRUE)
fhswf::fhswf_hooks()

# Load Packages
library(fhswf)
library(ggplot2)
```


# Einleitung

Geht es um Vorhersagen von Zeitreihen mit komplexen saisonalen Mustern, so sind wohl die bekanntesten statistischen Modellierungen Autoregressive Integrated Moving Average (ARIMA), Exponential Smoothing oder das Prohphet Modell. Im Bereich der Neuronalen Netze ist Long-Short-Term-Memory (LSTM), welches eine spezielle Art von Recurrent Neural Network RNN ist, das bekannte Neuronale Netz für komplexe Zeitreihen. Ein aktuell neuer Ansatz für Vorhersage von komplexen Zeitreihen ist ein weiteres RNN, das Liquid Neuronal Network (LNN). Es geht darum mit weniger Rechenleistung und geringerem Datensatz ein kompakteres Modell zu erzeugen, welches sich z.B. in Echtzeit an die Datenlage anpassen kann. So kann z.B. ein LSTM 100.000 - 1.000.000 Parameter haben, wobei das LNN mit bis zu 10.000 Parametern auskommt und ähnliche Resultate liefert. So ist eine Anbindung an elektronische Messgeräte oder Rasbery PI mit weniger Rechneleistung im Gegensatz zu LSTM möglich. In dieser Arbeit wird ein LNN auf einen Datensatz von der Stromerzeugung vom Unternehmen American Electric Power angewandt. Das Ziel ist es eine Vorhersage für den nächsten Tag "day-ahead" zu treffen. Bei den Neuronalen Netzen steht immer die Frage im Raum, welche Aktivierungsfunktion genutzt werden soll. Als Ergänzung zu dem LNN wird ein self learning activation function (SLAF) Modell implementiert. SLAF soll das vanishing gradient Problem dezimieren, aber auch eine approximierte Aktivierungsfunktion für das LNN finden. 


# Methodik 
Eine der bekanntesten Modelle für eine Zeitreihe ist das ARIMA Model.
Prophet als ein weiteres Benchmark-Model, welches sich an  komplexe Saisonalität
anpasst. 

## Liquid Time-Constant Neuronal Networks (LTCNNs)

Liquid Neural Networks, insbesondere LTCNNs, sind eine neuartige Form von neuronalen Netzen, die speziell für dynamische, zeitabhängige Daten entwickelt wurden. Im Gegensatz zu klassischen neuronalen Netzen, bei denen die Verbindungen zwischen den Neuronen statisch sind, passen sich die Verbindungen in LTCNs kontinuierlich über die Zeit an. Dies ermöglicht es den Modellen, dynamisch auf unterschiedliche Eingabeströme zu reagieren, was sie besonders geeignet für die Verarbeitung von zeitlichen Daten macht, wie z.B. in Zeitreihenanalysen oder bei Echtzeitanwendungen.

Der Kernmechanismus von LTCNs basiert auf der Verwendung von Differentialgleichungen zur Beschreibung der Neuronenaktivität. Jedes Neuron \( x_i(t) \) wird durch eine Differentialgleichung beschrieben, die seinen Zustand in Abhängigkeit von der Zeit und den Eingangsgrößen steuert:

\[
\frac{dx(t)}{dt} = -\left[\frac{1}{\tau} + f(x(t), I(t), t, \theta)\right] x(t) + f(x(t), I(t), t, \theta)A
\]

Hier ist \(x(t)\) der verborgene Zustand, \(\tau\) ist die Zeitkonstante, und \(f(x(t), I(t), t, \theta)\) ist eine Funktion, die sich mit der Eingabe \(I(t)\), der Zeit \(t\) und den Parametern \(\theta\) entwickelt.

Die Zeitkonstante \( \tau_i \) spielt eine zentrale Rolle, da sie bestimmt, wie schnell das Neuron auf Änderungen in den Eingangsdaten reagiert. Diese Konstante wird während des Trainings dynamisch angepasst, was den Begriff "Liquid" (flüssig) erklärt: Die Netzwerkkonfiguration ist nicht statisch, sondern verändert sich kontinuierlich im Laufe der Zeit:

\[
\tau_{\text{sys}} = \frac{\tau}{1 + \tau f(x(t), I(t), t, \theta)}.
\]

Um diese Netze zu trainieren, verwenden die Autoren eine Technik namens Backpropagation through Time (BPTT) in Kombination mit einem benutzerdefinierten Solver, um die Gleichungen des Netzes effizient zu lösen. Dieser Solver stellt ein Gleichgewicht zwischen Stabilität und Geschwindigkeit her. Die Aktualisierungsregel lautet wie folgt:

\[
x(t + \Delta t) = x(t) + \Delta t \frac{f(x(t), I(t), t, \theta)A}{1 + \Delta t \left(\frac{1}{\tau} + f(x(t), I(t), t, \theta)\right)}
\]

Die Architektur eines LTCNs ist relativ simpel aufgebaut: Jedes Neuron besitzt eine zeitabhängige Aktivierung, die durch die oben genannte Differentialgleichung gesteuert wird. Durch die ständige Anpassung der Zeitkonstanten können LTCNs flexibel auf Veränderungen in den Daten reagieren. Diese dynamischen Modelle sind besonders geeignet für komplexe, zeitlich veränderliche Muster.

In praktischen Tests übertrafen LTCs traditionelle Modelle wie Long Short-Time Memory Cells (LSTMs) und neuronale Ordinary Differential Equation (ODEs). Ob es um die Erkennung von Gesten oder die Vorhersage menschlicher Aktivitäten ging, in den meisten Fällen lieferten sie bessere Ergebnisse. Wie andere neuronale Netze haben sie jedoch aufgrund des Problems des verschwindenden Gradienten Schwierigkeiten mit langfristigen Abhängigkeiten. Außerdem benötigen sie im Vergleich zu einfacheren Modellen mehr Rechenressourcen.

Diese Ausgewogenheit von Flexibilität und Ausdruckskraft macht LTCs zu einer spannenden Entwicklung in der Welt der rekurrenten neuronalen Netze, auch wenn es noch einige Herausforderungen zu bewältigen gibt.

## Self Learnable Activation Functions (SLAFs)

Aktivierungsfunktionen haben einen großen Einfluss auf die Modellierung im Bereich der Neuronalen Netze. Dabei reagieren Neuronen auf verschiede Aktivierungsfunktionen unterschiedlich und beschreiben damit, ob ein Neuron gerade den Status "aktiv" oder "nicht aktiv" hat. Ist ein bestimmter Schwellwert in einer dieser Funktionen überschritten, so ändert das Neuron seinen Status. Die wohl bekanntesten Aktivierungsfunktionen sind sigmoid, relu, tanh und eine linearen Aktivierung. Dabei wurde nach \cite{Ramachandran2017} die swish-Funktion aufgedeckt: 

\[
f(x) = x * sigmoid(\beta * x)
\]

In der Arbeit von \cite{Ramachandran2017} wurde die swish-Funktion als die beste Aktivierungsfunktion empirisch bestimmt. Dabei wurde vor allem mit der ReLU-Funktion verglichen. 

## Auto Regressive-Moving Average (ARIMA)


## PROPHET: Forecasting at a scale
- Lagged Values <- ARIMA Bezug / ACF-PACF <- Vitali 
- Train / Testdatensatz <- egal wer, nur paar sätze


# Datengrundlage und Datenanalyse
Auch für Neuronales Netz muss Datenanalyse betrieben werden. Es ist wichtig den Datensatz komplett zu durchdringen und die Erkenntnisse aus der Analyse in der Implementierung des Neuronalen Netzen mit zu berücksichtigen. Der rohe Datensatz der Stromerzeugung hat eine stündliche Granularität in der Zeitspanne 01.10.2004 - 03.08.2018 (5054 Tage). Untersucht man den Datensatz auf Äquidistanz, so fallen Insgesamt 40 fehlende Beobachtungen in der stündlichen Auflösung. Die Abbildung \ref{fig:raw_AEP} repräsentiert den rohen Datensatz. Die fehlenden Werte (rot) wurden mit dem letzten vorhandenen Wert ersetzt. Dies ist damit zu begründen, da der Datensatz groß genug ist und es keine drastischen Auswirkungen auf das Neuronale Netz haben wird. Nach der säuberung der Daten hat der Datensatz korrekterweise 121.296 ($24*5054$) Beobachtungen. Ein positiver oder negativer Trend ist für die Stromerzeugung nicht zu erkennen.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/raw_AEP_MW.png}
\caption{Roher Datensatz zur Stromerzeugung (01.10.2004 - 03.08.2018) vom Unternehmen American Electric Power}
\label{fig:raw_AEP}
\end{figure}

Wird der Datensatz der Stromerzeugung in die einzelnen Jahre aufgespalten, so ist deutlich ein moustache-Muster (Schnurrbart) zu erkennen. Dieses Muster kann inder Abbildung \ref{fig:raw_years} entnommen werden.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/raw_years.png}
\caption{Stromerzeugung aufgespalten in einzelne Jahre}
\label{fig:raw_years}
\end{figure}

Auch die Verteilung der Daten kann eine Rolle bei der Modellierung der Neuronalen Netze spielen. Die Stromerzeugung besitzt ein minimalen Wert von 9581 MW, einen maximalen Wert von 25.695MW und einer Durchschnitt von 15.499 MW. Die Abbildung \ref{fig:histogram} zeigt die Verteilung der Stromerzeugung (Normalverteilt). 

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/histogram.png}
\caption{Verteilung der Stromerzeugung}
\label{fig:histogram}
\end{figure}

Wird der Datensatz pro Stunde aggregiert, so ist auch ein weiteres Muster innerhalb der einzelnen Tage zu erkennen. So hat ist die Stromerzeugung in der Nacht (~23:00 - 08:00) runtergefahren und in der Tageszeit (~08:00 - 23:00) hochgefahren. Die Abbildung \ref{fighour_boxplot} stellt dabei die aggregierte Darstellung für jede Stunde dar.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/hour_boxplot.png}
\caption{Stündlich aggregierte Darstellung der Stromerzeugung}
\label{fig:hour_boxplot}
\end{figure}

Für ein tieferes Verständnis ist es sinnvoll die Wochentage, Werktage und die Feiertage zu betrachten. Als erstes die wöchentliche Betrachtung der Stromerzeugung. Die Abbildung \ref{fig:weekday_boxplot} stellt dabei die einzelnen Wochentage über die Jahre dar. Anzumerken ist, dass der Samstag und der Sonntag sich von den regulären Wochentagen in der Stromerzeugung unterscheiden. So haben die beiden Tage im Durchschnitt ca. 2000 MW weniger an Stromerzeugung.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/weekday_boxplot.png}
\caption{Wöchentlich aggregierte Darstellung der Stromerzeugung}
\label{fig:weekday_boxplot}
\end{figure}

Auch die einzelnen Feiertage unterscheiden sich von einem regulären Tag. In der Abbildung \ref{fig:holiday_boxplot} sind die einzelnen Feiertage abgebildet. Ähnliche wie ein Wochenende sinkt an den meisten Feiertagen die Stromerzeugung. Bis auf den Martin Luther Kind, Jr. Day, New Year's Day, Veterans Day und den Washington's Birthday verhalten sich die Feirtage wie ein Wochenende unter der Betrachtung vom Durchschnitt.  

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/holiday_boxplot}
\caption{Feiertage, aggregierte Darstellung der Stromerzeugung}
\label{fig:holiday_boxplot}
\end{figure}

So kann eine dummy variable erzeugt werden, die die Feiertage, aber auch die Wochenenden beinhaltet. Die Abbildung \ref{fig:workday_histogram} stellt dabei die Verteilung der Werktage und der nicht Werktage dar. Eine verschiebung für die nicht Werktage in eine geringere Stromerzeugung.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/workday_histogram}
\caption{Verteilung der Werktage und nicht Werktage}
\label{fig:workday_histogram}
\end{figure}

Um den Begriff der komplexen saisonalität zu verdeutlichen zeigt die Abbildung \ref{fig:acf} die Autokorrelation. Dabei werden die aktuellen Beobachtungen mit den Vergangenen gegenübergestellt und die Korrelation zwischen diesen ermittelt. So z.B. für lag 12 wird ein shift des Datensatzes um 12 Zeiteinheiten nach links verschoben. Es wird die Korrelation zwischen dem vorschebnen Datensatz und dem Originalen ermittelt. So in der Abbildung \ref{fig:acf} eine komplexe saisonalität, da es sowohl tägliche, wöchentliche aber auch jährliche Muster im Datensatz zu finden sind.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/autocorrelation}
\caption{Autokorrelation für die Stromerzeugung}
\label{fig:acf}
\end{figure}

Als letzter Punkt in der Datenanalyse ist die Betrachtung der verschiedenen vergangenen Beobachtungen. Durchschnitt der letzten Woche, Minimum vom letzten Tag, Maximum vom letzten Tag und Durchschnitt der letzten zwei Tage. Die Abbildung \ref{fig:corr} stellt dabei die Korrelation zwischen den einzelnen Werten dar. Es ist zwischen TRUE (Werktag) und FALSE (kein Werktag) zu unterscheiden. Eine mittelstarke (ca. 0.5) Korrelation für die Stromerzeugung und die unterschiedlichen Variablen.


\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plots/corr.png}
\caption{Korrelation zwischen den vergangenen Beobachtungen und der Stromerzeugung}
\label{fig:corr}
\end{figure}

- Statistische Methoden <- Vitali 
- Feature Engineering <- Vitali (Analytisch) & Vlad (PCA)
- "Calender Effect" <- Vitali

# Implementierung

## SLAF Integration in LTC Nodes

Wie schon erwähnt, die Swish Funktion wird in diesem Projekt anstatt der klassischen Sigmoid-Aktiviserungsfunktion angewendet. Wir erweitern die Funktionalität von "NCPs" Package um das Benutzen von Swish Funktion zu ermöglichen.

Die Aktivierungsfunktion wird ersetzt durch das Einstellen von dem optionalen boolischen Parameter `use_swish_activation` bei der "LTC" Modelle Definition:

```
class LTC(nn.Module):
    def __init__(
        self,
        input_size: int,
        units,
        return_sequences: bool = True,
        batch_first: bool = True,
        mixed_memory: bool = False,
        input_mapping="affine",
        output_mapping="affine",
        ode_unfolds=6,
        epsilon=1e-8,
        implicit_param_constraints=True,
        use_swish_activation=False,
    ):
      ...
```

Der Parameter wird später in jede LTC Knoten übergeben, welcher durch eine weitere Python Klasse repräsentiert ist:

```
class LTCCell(nn.Module):
    def __init__(
        self,
        wiring,
        in_features=None,
        input_mapping="affine",
        output_mapping="affine",
        ode_unfolds=6,
        epsilon=1e-8,
        implicit_param_constraints=False,
        use_swish_activation=False,
    ):
      ...
```

Falls der Parameter als True gesetzt ist, wird einen neuen Parameter für PyTorch Modelle definiert, welcher ein zufälliger Skalarwert im Zahlenbereich von `0,5` bis `1,5` ist:

```
def add_weight(self, name, init_value, requires_grad=True):
    param = torch.nn.Parameter(init_value, requires_grad=requires_grad)
    self.register_parameter(name, param)
    return param

def _get_init_value(self, shape, param_name):
    minval, maxval = self._init_ranges[param_name]
    if minval == maxval:
        return torch.ones(shape) * minval
    else:
        return torch.rand(*shape) * (maxval - minval) + minval

if self._use_swish_activation:
    self._params["swish_beta"] = self.add_weight(
        "swish_beta",
        init_value=self._get_init_value(
            (1,), "swish_beta"
        ),
    )
```

Der Skalar wird dann später verwendet, um einen Matrizen Produkt zu berechnen:

```
def _sigmoid(self, v_pre, mu, sigma, beta):
    v_pre = torch.unsqueeze(v_pre, -1)  # For broadcasting
    mues = v_pre - mu
    x = sigma * mues
    if self._use_swish_activation:
        return x * torch.sigmoid(beta * x)
    return torch.sigmoid(x)
```

Der Parameter ist einer Anpassung bei einer Error Backpropagation fällig, und darüber hinaus, darf von dem initialen Wert während des Trainings unterscheiden.

## Feature Engineering

Insgesamt 
Neben der initialen Variablen, die aus der Datenanalyse erzeugt wurden, hinzufügen wir zwei weitere Spalten, die wöchentliche und jährliche Werten Verzögerung der abhängigen Variable einführen. Für Time-Series Daten ist eine normale Praktik, die wir hier auch verfolgen. Insgesamt 
ergeben sich 17, die einen möglichen Einfluss auf das Modell haben können.

Wegen einer großen Dimensionalität des Datensatzes, möchten wir mithilfe der PCR-Analyse zeigen, wie viele Variablen man benötigt um die Varianz der Daten zu erklären.

"WorkDay","Mo","Di","Mi","Do","Fr","Sa","Holiday","LastDayWasNotWorkDay","LastDayWasNotWorkDayAndNowWorkDay","NextDayIsNotWorkDayAndNowWorkDay","LastDayWasHolodiayAndNotWeekend","NextDayIsHolidayAndNotWeekend","MeanLastWeek","MeanLastTwoDays","MaxLastOneDay","MinLastOneDay"

## Training

Durch die `pytorch`-Implementierung lässt sich das Training sowohl auf einer CPU, als auch einer GPU ohne große Anpassungen stattfinden. Um das Training stabiler zu gestalten, waren das Hardware von FH-SWF Cluster genutzt.

Das Training lässt sich sowohl mittels cmd-Befehl als auch Jupyter Notebook anstoßen, allein nur die Parameter in `config.py` sind entscheidend:

```
PATH = "data/csv/AEP_hourly_cleaned.csv"
STATION = "AEP_MW"
FEATURES_LIST = [
    "WorkDay",
    "LastDayWasHolodiayAndNotWeekend",
    "NextDayIsHolidayAndNotWeekend",
    "MeanLastWeek",
    "MeanLastTwoDays",
    "MaxLastOneDay",
    "MinLastOneDay"
]
FEATURES_2_SCALE = [
    "value",
    "MeanLastWeek",
    "MeanLastTwoDays",
    "MaxLastOneDay",
    "MinLastOneDay"
]

YEAR_SHIFT = 365
WEEK_SHIFT = 7
VALUES_PER_DAY = 24
FILTER_DT_FROM = "2014-01-01 00:00:00"
FILTER_DT_TILL = "2017-01-01 00:00:00"

GRID_SEARCH = True

BATCH_SIZE = 7
NUM_WORKERS = 128
NUM_LNN_UNITS = [16, 8, 32]

USE_SWISH_ACTIVATION = [False, True]
INIT_LR = [0.01, 0.0001]
NUM_EPOCHS = [10, 50, 100]
```

Außer Datenpfad, Energiestation und Liste relevanten Features, lässt sich auch einen Grid Search mit dem Parameter-Pool einstellen. Die wichtigsten Hyperparameter, die für LNN eine Rolle spielen, sind es Anzahl von Knoten, eine initiale Learning Rate und Anzahl von Epochen. Durch die 
Konfiguration kann man auch die Schranken für den Trainingsdatensatz definieren, in unserem Fall haben wir uns für 3 Jahren Daten zwischen 2014 und 2017 entschieden. Für die Auswertung nutzen wir die Daten aus danach folgender Woche.

Bevor das eigentliche Training startet, werden die Daten normalisiert auf Bereich zwischen 0 und 1 mit dem `sklearn.processing.MinMaxScaler` Objekt. Für die Evaluation der Ergebnisse bzw. produktives Nutzen kann der Skaler die Daten denormalizieren. Welche Merkmale normaliziert werden müssen, kann auch mittels `FEATURES_2_SCALE` eingestellt werden, da z.B. die boolische Variablen brauchen keine Skalierung.

Für das unkomplizierte Training wird die `Trainer` Klasse aus der `pytorch_lightning` Bibliothek benutzt, welche einen Data Scientist vom Boiler Code befreit.

Aufgrund von der Datenmenge und angeblich einer schlechter Optimizierung von `ncps` Package ergeben sich Stunden um ein Modell mehr als 100 Epochen lang trainieren zu lassen. Wir sind der Weg gegangen, der Grid Search auf 10 Epochen pro Paramterkombination zu begrenzen, um den Potential von den Modellen analytisch zu betrachten. Dafür haben wir insgesamt 36 Modellen miteinander verglichen, zuerst wurde für jeden Kandidaten einen `Mean Average Percentage Error` berechnet und eine grafische Darstellung der Vorheragen zum Rat gezogen. Die meist vielversprechende Parameterkombinationen wurden dann für längeres Training (400 Epochen) ausgewählt.

Nach den 10 Epochen haben die besten Modelle einen `MAPE Loss` zwischen 10% und 15% gezeigt:

HERE 3 PLOTS WITH SOME EXPLAINING


Während des Trainings haben wir immer meistens eine gestrichelte Linie mit rückläufiges Tendenz gesehen. Ab und zu waren es enorm höhe Trainingsverluste, die wir nicht erklären können:

PLOT

Die Ergebnisse von den drei Modellen werden in dem nächsten Kapitel näher diskutiert.

# Vorhersage Ergebnisse
- Metriks <- MAE / MAPE (X) / LOSS-FUNCTION <- Vlad
- Plots der Ergebnisse <- Vlad

# Fazit <- Zusammen alles
- Probleme
- Lösungen
- Zusammenfassung
- Schlussfolgerung und letztes Wort


