# Liquid Time Constant Neural Network enhanced with Self Learnable Activation Function

## Disclaimer
The project is forked from "Neural Circuit Policies" (ncps) [ðŸ“– Docs](https://ncps.readthedocs.io/en/latest/index.html). The analysis made in this repository were conducted study purpose only.

## Description
Our project is an experiment with Liquid Neural Networks. We extend the functionality of the `LTCCell` by introducing a Self-Learning Activation Function, i.e. swish function, to fight the vanishing gradient problem well known to all Deep Learning architectures.

We use the [Kaggle dataset](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption) to test the performance of introducing swish function as the main activation function. Besides, we benchmark the achieved predictions with Liquid Time Constant Neural Networks against well-known statistical models, like ARIMA. The detailed research and conducted results can be found in `examination/examination.pdf`. (currently available only in german language)

## Project Structure
```
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ csv            <- Time-Series data in csv format (one csv - one station).
â”‚Â Â  â””â”€â”€ parquet        <- Time-Series data in parquet format (one file for all stations).
â”‚
â”œâ”€â”€ ncps               <- Forked code with minor changes (see Disclaimer)
â”‚
â”œâ”€â”€ examination        <- Paper where the final project is fully described. (Currently in german only)
â”‚
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks. Training, Evaluation and PCA Analysis notebooks can be found here.
â”‚
â”œâ”€â”€ project            <- Python utility code used for data preparation and training.
â”‚
â”œâ”€â”€ analysis            <- Folder containing all stuff related to the data analysis made in R.
â”‚   â”œâ”€â”€ R               <- R Scripts used for data cleaning and feature engineering.
â”‚Â Â  â””â”€â”€ plots           <- Generated graphics and figures to be used in data analysis.
â”‚
â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
â”‚                         generated with `pip freeze > requirements.txt`
â”‚
â”‚â”€â”€ run.py            <- `main` python file for training the LNN models.
â”‚â”€â”€ run.py            <- Configuration file containing data related variables as well as LNN hyperparameters.
â””â”€â”€ log               <- Logging information automatically generated by pytorch_lightning.
```

## Installation
Please use `poetry.lock` or `requirements.txt` for creating the python virtual environment for the project:

### CPU
```bash
poetry install -E cpu --with cpu
// or
python -m pip install -r ./requirements_cpu.txt
```

### GPU
```bash
poetry install -E cuda --with cuda
// or
python -m pip install -r ./requirements_gpu.txt
```

## Usage
To train the model by using the commad shell:
```bash
poetry run python -m run.py
// or
python -m run.py
```
The output (checkpoints) containing the models, will be placed into `pl_checkpoints/` directory. Feel free use other location by changing the variable in `config.py`.

You also may use jupyter notebooks provided in `notebooks/` for training and evaluation purposes.

If you would like to only experiment with already trained models, please find those in `models.tar`. You can use them by executing following python code:

```python
# best_lnn_slaf_model
from ncps.torch import LTC
from ncps.wirings import AutoNCP
from project.model import SequenceLearner

wiring = AutoNCP(8, 1)  # 8 units, 1 motor neuron

ltc_model = LTC(
    in_features,
    wiring,
    batch_first=True,
    use_swish_activation=True
)

trainer = SequenceLearner.load_from_checkpoint(path)
model = trainer.model
model.to(device)
model.eval()

# your input
X = ...

preds = model(X).detach().numpy()
```

```python
# best_lnn_model
from ncps.torch import LTC
from ncps.wirings import AutoNCP
from project.model import SequenceLearner

wiring = AutoNCP(32, 1)  # 8 units, 1 motor neuron

ltc_model = LTC(
    in_features,
    wiring,
    batch_first=True,
    use_swish_activation=False
)

trainer = SequenceLearner.load_from_checkpoint(path)
model = trainer.model
model.to(device)
model.eval()

# your input
X = ...

preds = model(X).detach().numpy()
```

## Authors & Contributing
Vitali Krilov & Vladislav Stasenko (both FHSWF students) authored this GitHub repository.

We belive that experimenting with LNN is a highly interesting topic, therefore further community help and ideas is highly welcome!
